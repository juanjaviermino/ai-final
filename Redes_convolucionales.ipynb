{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neuronales convolucionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importar librerías: Instalar tensorflow, por si acaso\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrucción del modelo para la CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inicializar la CNN (Crear la matriz inicial)\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convolución\n",
    "\n",
    "# 32: 32 matrices que sacará el proceso de convolución. \n",
    "# 3 y 3: dimensiones de la matriz del filtro. \n",
    "# Input shape: 64, 64, 3: resolución de filas por columnas por colores. (3 colores básicos en este caso)\n",
    "# activation: función de activación (criterio/esquema).\n",
    "\n",
    "classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = \"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Max Pooling y Flattening\n",
    "\n",
    "# 2, 2: dimensión de las porciones que se van a tomar de las matrices de convolución\n",
    "# Flatten: transformar estas matrices en vectores\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full connection\n",
    "\n",
    "# Crear las capas ocultas. \n",
    "# La primera capa se hace con Relu: 128 es una potencia de 2. 128 es la cantidad de neuronas de la primera capa oculta.\n",
    "# el 128 se saca multiplicando la cantidad de matrices de entrada por el número de capas de salida.\n",
    "# Peso inicial igualitario: 100% dividido para el número de entradas. \n",
    "\n",
    "# La segunda capa tiene la función sigmoide de activación. \n",
    "\n",
    "classifier.add(Dense(units = 128, activation = \"relu\"))\n",
    "classifier.add(Dense(units = 1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilación neuronal\n",
    "\n",
    "# Adam: función de optimización para ajustar los pesos de la red.\n",
    "# función de pérdida: entropía binaria. (Tiende a cero o uno)\n",
    "# metrics: métrica utilizada para evaluar el rendimiento del modelo.\n",
    "\n",
    "classifier.compile(optimizer = \"adam\",  # El optimizador Adam se usa para ajustar los pesos de la red de manera eficiente.\n",
    "                   loss = \"binary_crossentropy\",  # La función de pérdida de entropía cruzada binaria se utiliza para problemas de clasificación binaria. Esta función mide la disimilitud entre las predicciones del modelo y las etiquetas reales, donde los resultados son 0 o 1.\n",
    "                   metrics = [\"accuracy\"])  # La precisión es la métrica seleccionada para evaluar el rendimiento del modelo durante el entrenamiento y la evaluación. Indica el porcentaje de predicciones correctas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hasta aquí está armada la arquitectura de la CNN. Comenzamos con el entrenamiento en la segunda parte: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías e inicialización del batch size. \n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "batch_size = 32 ## 50% del input shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de generadores de datos para el entrenamiento y la validación\n",
    "\n",
    "# Generador de datos para el conjunto de entrenamiento\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Escala los valores de los píxeles de 0-255 a 0-1. Esto es necesario para normalizar las imágenes, facilitando el entrenamiento del modelo.\n",
    "    shear_range=0.2,  # Aplica transformaciones de corte (shear) a las imágenes. Esta transformación distorsiona las imágenes a lo largo del eje x o y, lo que ayuda a aumentar la diversidad del conjunto de datos.\n",
    "    zoom_range=0.2,  # Aplica un rango de zoom aleatorio a las imágenes. Esto significa que algunas imágenes se acercarán y otras se alejarán, ayudando a la red a generalizar mejor.\n",
    "    horizontal_flip=True  # Invierte horizontalmente las imágenes aleatoriamente. Esto aumenta la diversidad del conjunto de datos y ayuda a la red a aprender características independientes de la orientación horizontal.\n",
    ")\n",
    "\n",
    "# Generador de datos para el conjunto de prueba/validación\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255  # Escala los valores de los píxeles de 0-255 a 0-1. A diferencia del conjunto de entrenamiento, aquí solo se aplica la normalización sin aumentación de datos, ya que queremos evaluar el rendimiento del modelo en datos que no se han visto afectados por transformaciones.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 85 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_datagen.flow_from_directory(\n",
    "    'dataset/training_set',  # Ruta del conjunto de entrenamiento\n",
    "    target_size=(64, 64),  # Todas las imágenes serán redimensionadas a 64x64\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'  # Ya que usamos clasificación binaria\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_dataset = test_datagen.flow_from_directory(\n",
    "    'dataset/test_set',  # Ruta del conjunto de prueba\n",
    "    target_size=(64, 64),  # Todas las imágenes serán redimensionadas a 150x150\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6782 - accuracy: 0.5472WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20 batches). You may need to use the repeat() function when building your dataset.\n",
      "2/2 [==============================] - 0s 332ms/step - loss: 0.6782 - accuracy: 0.5472 - val_loss: 0.6987 - val_accuracy: 0.4937\n",
      "Epoch 2/25\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.7013 - accuracy: 0.4717\n",
      "Epoch 3/25\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.6578 - accuracy: 0.5094\n",
      "Epoch 4/25\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.6599 - accuracy: 0.6226\n",
      "Epoch 5/25\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.6529 - accuracy: 0.6981\n",
      "Epoch 6/25\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.6515 - accuracy: 0.7188\n",
      "Epoch 7/25\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.6271 - accuracy: 0.7812\n",
      "Epoch 8/25\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6466 - accuracy: 0.6604\n",
      "Epoch 9/25\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6245 - accuracy: 0.6792\n",
      "Epoch 10/25\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6051 - accuracy: 0.7358\n",
      "Epoch 11/25\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6260 - accuracy: 0.6792\n",
      "Epoch 12/25\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.5654 - accuracy: 0.7969\n",
      "Epoch 13/25\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5879 - accuracy: 0.7031\n",
      "Epoch 14/25\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5803 - accuracy: 0.7344\n",
      "Epoch 15/25\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.5726 - accuracy: 0.7656\n",
      "Epoch 16/25\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.6170 - accuracy: 0.7031\n",
      "Epoch 17/25\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.5645 - accuracy: 0.7188\n",
      "Epoch 18/25\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.5870 - accuracy: 0.6875\n",
      "Epoch 19/25\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5675 - accuracy: 0.7656\n",
      "Epoch 20/25\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5249 - accuracy: 0.8113\n",
      "Epoch 21/25\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.5306 - accuracy: 0.7736\n",
      "Epoch 22/25\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.5187 - accuracy: 0.8113\n",
      "Epoch 23/25\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.5306 - accuracy: 0.7925\n",
      "Epoch 24/25\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4963 - accuracy: 0.7736\n",
      "Epoch 25/25\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.5215 - accuracy: 0.8302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2911864bc50>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "\n",
    "classifier.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=2,\n",
    "    epochs=25,\n",
    "    validation_data=test_dataset,\n",
    "    validation_steps=20\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
