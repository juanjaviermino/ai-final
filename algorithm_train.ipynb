{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Final: \n",
    "### Reconocimiento de emociones con visión por computadora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías y establecer las rutas de los archivos\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "train_dir = 'C:/Users/jmino/OneDrive/Escritorio/archive/images/images/train'\n",
    "validation_dir = 'C:/Users/jmino/OneDrive/Escritorio/archive/images/images/validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargar imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28821 images belonging to 7 classes.\n",
      "Found 7066 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Definir ImageDataGenerator para la augmentación y normalización de datos en el conjunto de entrenamiento\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,         # Escala los valores de los píxeles a un rango de 0 a 1\n",
    "    rotation_range=40,       # Rango de rotación aleatoria en grados (aumentado a 40)\n",
    "    width_shift_range=0.3,   # Rango de desplazamiento horizontal aleatorio como una fracción del ancho total (aumentado a 0.3)\n",
    "    height_shift_range=0.3,  # Rango de desplazamiento vertical aleatorio como una fracción de la altura total (aumentado a 0.3)\n",
    "    shear_range=0.3,         # Rango de transformación de corte (aumentado a 0.3)\n",
    "    zoom_range=0.3,          # Rango de zoom aleatorio (aumentado a 0.3)\n",
    "    horizontal_flip=True,    # Voltea las imágenes horizontalmente de forma aleatoria\n",
    "    fill_mode='nearest'      # Estrategia de relleno para los nuevos píxeles que se crean al aplicar las transformaciones\n",
    ")\n",
    "\n",
    "# Definir ImageDataGenerator para la normalización de datos en el conjunto de validación\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)  # Solo escala los valores de los píxeles a un rango de 0 a 1\n",
    "\n",
    "# Cargar y preprocesar el conjunto de entrenamiento\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(64, 64),  # Ajusta el tamaño de las imágenes a 64x64 píxeles (puede ajustarse según sea necesario)\n",
    "    batch_size=32,              # Tamaño del lote\n",
    "    class_mode='categorical',  # Modo de clasificación: las etiquetas son categóricas\n",
    "    color_mode='grayscale' # Modo de color: imágenes en escala de grises\n",
    ")\n",
    "\n",
    "# Cargar y preprocesar el conjunto de validación\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(64, 64),  # Ajusta el tamaño de las imágenes a 64x64 píxeles (puede ajustarse según sea necesario)\n",
    "    batch_size=32,         # Tamaño del lote\n",
    "    class_mode='categorical',  # Modo de clasificación: las etiquetas son categóricas\n",
    "    color_mode='grayscale'  # Modo de color: imágenes en escala de grises\n",
    ")\n",
    "\n",
    "# Nota: Las imágenes de entrenamiento y validación tienen aproximadamente 2 KB, por lo que tienen una baja resolución.\n",
    "# Es importante tener en cuenta que la baja resolución puede afectar la precisión del modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición de la CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la arquitectura del modelo CNN ajustada para imágenes en blanco y negro\n",
    "# Las redes neuronales convolucionales (CNN) son particularmente efectivas para el reconocimiento de imágenes debido a su capacidad para captar características espaciales y patrones en los datos de entrada.\n",
    "\n",
    "model = Sequential([\n",
    "    # Primera capa convolucional con 32 filtros de tamaño 3x3, función de activación ReLU y una forma de entrada de 64x64x1 (imágenes en blanco y negro)\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),  # Nota: El número de canales es 1 para escala de grises\n",
    "    BatchNormalization(), # (Añadido en la segunda prueba)\n",
    "    # Aplicación de Max Pooling con una ventana de 2x2 para reducir la dimensionalidad y retener las características más importantes\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Segunda capa convolucional con 64 filtros de tamaño 3x3 y función de activación ReLU\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(), # (Añadido en la segunda prueba)\n",
    "    # Aplicación de Max Pooling con una ventana de 2x2 para reducir aún más la dimensionalidad\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Tercera capa convolucional con 128 filtros de tamaño 3x3 y función de activación ReLU\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    BatchNormalization(), # (Añadido en la segunda prueba)\n",
    "    # Aplicación de Max Pooling con una ventana de 2x2 para reducir nuevamente la dimensionalidad\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # (Nueva capa añadida en la segunda prueba)\n",
    "    Conv2D(256, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Capa de Flattening para convertir la matriz resultante de la última capa de pooling en un vector\n",
    "    Flatten(),\n",
    "\n",
    "    # Capa densa totalmente conectada con 128 neuronas y función de activación ReLU\n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(0.001)), # Antes: (Dense(256, activation='relu'),)\n",
    "\n",
    "    # Capa de Dropout con una tasa de 0.5 para prevenir el sobreajuste durante el entrenamiento\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Capa de salida con 7 neuronas (una por cada clase) y función de activación softmax para obtener probabilidades de clasificación\n",
    "    Dense(7, activation='softmax')  # Suponiendo 7 clases: angry, disgust, fear, happy, neutral, sad, surprise\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilación\n",
    "\n",
    "# Compilar el modelo\n",
    "# La compilación del modelo es un paso necesario antes de entrenarlo. Aquí se especifican el optimizador, \n",
    "# la función de pérdida y las métricas que se utilizarán para evaluar el rendimiento del modelo durante el entrenamiento.\n",
    "\n",
    "# Parámetro optimizer\n",
    "# Se ha elegido el optimizador Adam (Adaptive Moment Estimation) porque combina las ventajas de los optimizadores \n",
    "# AdaGrad y RMSProp. Adam adapta la tasa de aprendizaje para cada parámetro del modelo, lo que puede conducir a una \n",
    "# convergencia más rápida y eficiente. Es conocido por funcionar bien en una amplia gama de problemas de aprendizaje profundo.\n",
    "\n",
    "# Parámetro learning_rate\n",
    "# La tasa de aprendizaje se ha fijado en 0.001. Este valor es un buen punto de partida para muchos problemas. \n",
    "# Un valor más alto podría hacer que el modelo converja demasiado rápido a un mínimo local, mientras que un valor \n",
    "# más bajo podría hacer que el entrenamiento sea innecesariamente lento. Ajustar la tasa de aprendizaje es crucial para \n",
    "# el rendimiento del modelo.\n",
    "\n",
    "# Parámetro loss\n",
    "# La función de pérdida utilizada es 'categorical_crossentropy'. Esta es la elección estándar para problemas de \n",
    "# clasificación multiclase, donde las clases son mutuamente excluyentes. En este caso, estamos clasificando imágenes \n",
    "# en una de las 7 categorías de expresiones faciales (angry, disgust, fear, happy, neutral, sad, surprise). \n",
    "# La 'categorical_crossentropy' mide la diferencia entre la distribución de las clases verdaderas y las predichas por el modelo.\n",
    "\n",
    "# Parámetro metrics\n",
    "# La métrica utilizada es 'accuracy'. La precisión es una métrica intuitiva y fácil de interpretar que mide \n",
    "# la fracción de predicciones correctas. Es especialmente útil para evaluar el rendimiento de modelos de clasificación.\n",
    "\n",
    "# Antes: model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "900/900 [==============================] - 91s 101ms/step - loss: 1.1797 - accuracy: 0.5662 - val_loss: 1.1552 - val_accuracy: 0.5875\n",
      "Epoch 2/40\n",
      "900/900 [==============================] - 84s 94ms/step - loss: 1.1838 - accuracy: 0.5680 - val_loss: 1.0923 - val_accuracy: 0.6016\n",
      "Epoch 3/40\n",
      "900/900 [==============================] - 85s 94ms/step - loss: 1.1808 - accuracy: 0.5688 - val_loss: 1.0568 - val_accuracy: 0.6271\n",
      "Epoch 4/40\n",
      "900/900 [==============================] - 87s 97ms/step - loss: 1.1772 - accuracy: 0.5676 - val_loss: 1.1064 - val_accuracy: 0.5938\n",
      "Epoch 5/40\n",
      "900/900 [==============================] - 81s 90ms/step - loss: 1.1803 - accuracy: 0.5655 - val_loss: 1.0894 - val_accuracy: 0.6055\n",
      "Epoch 6/40\n",
      "900/900 [==============================] - 80s 88ms/step - loss: 1.1779 - accuracy: 0.5675 - val_loss: 1.2574 - val_accuracy: 0.5459\n",
      "Epoch 7/40\n",
      "900/900 [==============================] - 79s 88ms/step - loss: 1.1792 - accuracy: 0.5667 - val_loss: 1.1213 - val_accuracy: 0.5973\n",
      "Epoch 8/40\n",
      "900/900 [==============================] - 81s 90ms/step - loss: 1.1751 - accuracy: 0.5711 - val_loss: 1.1813 - val_accuracy: 0.5653\n",
      "Epoch 9/40\n",
      "900/900 [==============================] - 80s 88ms/step - loss: 1.1807 - accuracy: 0.5660 - val_loss: 1.0439 - val_accuracy: 0.6276\n",
      "Epoch 10/40\n",
      "900/900 [==============================] - 80s 88ms/step - loss: 1.1732 - accuracy: 0.5700 - val_loss: 1.0677 - val_accuracy: 0.6162\n",
      "Epoch 11/40\n",
      "900/900 [==============================] - 82s 91ms/step - loss: 1.1804 - accuracy: 0.5703 - val_loss: 1.0562 - val_accuracy: 0.6233\n",
      "Epoch 12/40\n",
      "900/900 [==============================] - 82s 91ms/step - loss: 1.1725 - accuracy: 0.5722 - val_loss: 1.0879 - val_accuracy: 0.6109\n",
      "Epoch 13/40\n",
      "900/900 [==============================] - 80s 89ms/step - loss: 1.1645 - accuracy: 0.5754 - val_loss: 1.0805 - val_accuracy: 0.6170\n",
      "Epoch 14/40\n",
      "900/900 [==============================] - 77s 86ms/step - loss: 1.1737 - accuracy: 0.5703 - val_loss: 1.0377 - val_accuracy: 0.6374\n",
      "Epoch 15/40\n",
      "900/900 [==============================] - 79s 88ms/step - loss: 1.1744 - accuracy: 0.5722 - val_loss: 1.1952 - val_accuracy: 0.5651\n",
      "Epoch 16/40\n",
      "900/900 [==============================] - 92s 102ms/step - loss: 1.1725 - accuracy: 0.5731 - val_loss: 1.1324 - val_accuracy: 0.5878\n",
      "Epoch 17/40\n",
      "900/900 [==============================] - 162s 180ms/step - loss: 1.1663 - accuracy: 0.5763 - val_loss: 1.0679 - val_accuracy: 0.6224\n",
      "Epoch 18/40\n",
      "900/900 [==============================] - 151s 167ms/step - loss: 1.1722 - accuracy: 0.5683 - val_loss: 1.0641 - val_accuracy: 0.6264\n",
      "Epoch 19/40\n",
      "900/900 [==============================] - 129s 144ms/step - loss: 1.1751 - accuracy: 0.5717 - val_loss: 1.0869 - val_accuracy: 0.6131\n",
      "Epoch 20/40\n",
      "900/900 [==============================] - 144s 160ms/step - loss: 1.1666 - accuracy: 0.5749 - val_loss: 1.0616 - val_accuracy: 0.6210\n",
      "Epoch 21/40\n",
      "900/900 [==============================] - 132s 146ms/step - loss: 1.1682 - accuracy: 0.5722 - val_loss: 1.0528 - val_accuracy: 0.6246\n",
      "Epoch 22/40\n",
      "900/900 [==============================] - 153s 170ms/step - loss: 1.1687 - accuracy: 0.5730 - val_loss: 1.0593 - val_accuracy: 0.6229\n",
      "Epoch 23/40\n",
      "900/900 [==============================] - 126s 140ms/step - loss: 1.1615 - accuracy: 0.5747 - val_loss: 1.1019 - val_accuracy: 0.6097\n",
      "Epoch 24/40\n",
      "900/900 [==============================] - 118s 131ms/step - loss: 1.1609 - accuracy: 0.5779 - val_loss: 1.0763 - val_accuracy: 0.6163\n",
      "Epoch 25/40\n",
      "900/900 [==============================] - 127s 141ms/step - loss: 1.1682 - accuracy: 0.5736 - val_loss: 1.0794 - val_accuracy: 0.6101\n",
      "Epoch 26/40\n",
      "900/900 [==============================] - 131s 145ms/step - loss: 1.1663 - accuracy: 0.5747 - val_loss: 1.1247 - val_accuracy: 0.5864\n",
      "Epoch 27/40\n",
      "900/900 [==============================] - 136s 152ms/step - loss: 1.1691 - accuracy: 0.5745 - val_loss: 1.1060 - val_accuracy: 0.5982\n",
      "Epoch 28/40\n",
      "900/900 [==============================] - 130s 144ms/step - loss: 1.1771 - accuracy: 0.5724 - val_loss: 1.0625 - val_accuracy: 0.6233\n",
      "Epoch 29/40\n",
      "900/900 [==============================] - 130s 144ms/step - loss: 1.1612 - accuracy: 0.5742 - val_loss: 1.1126 - val_accuracy: 0.6011\n",
      "Epoch 30/40\n",
      "900/900 [==============================] - 123s 137ms/step - loss: 1.1651 - accuracy: 0.5762 - val_loss: 1.0809 - val_accuracy: 0.6111\n",
      "Epoch 31/40\n",
      "900/900 [==============================] - 132s 147ms/step - loss: 1.1605 - accuracy: 0.5755 - val_loss: 1.0570 - val_accuracy: 0.6205\n",
      "Epoch 32/40\n",
      "900/900 [==============================] - 121s 134ms/step - loss: 1.1625 - accuracy: 0.5757 - val_loss: 1.0771 - val_accuracy: 0.6199\n",
      "Epoch 33/40\n",
      "900/900 [==============================] - 127s 141ms/step - loss: 1.1654 - accuracy: 0.5742 - val_loss: 1.0652 - val_accuracy: 0.6186\n",
      "Epoch 34/40\n",
      "900/900 [==============================] - 141s 157ms/step - loss: 1.1609 - accuracy: 0.5719 - val_loss: 1.0561 - val_accuracy: 0.6197\n",
      "Epoch 35/40\n",
      "900/900 [==============================] - 136s 151ms/step - loss: 1.1619 - accuracy: 0.5766 - val_loss: 1.1306 - val_accuracy: 0.6114\n",
      "Epoch 36/40\n",
      "900/900 [==============================] - 126s 140ms/step - loss: 1.1623 - accuracy: 0.5765 - val_loss: 1.0730 - val_accuracy: 0.6236\n",
      "Epoch 37/40\n",
      "900/900 [==============================] - 93s 103ms/step - loss: 1.1638 - accuracy: 0.5722 - val_loss: 1.1329 - val_accuracy: 0.5852\n",
      "Epoch 38/40\n",
      "900/900 [==============================] - 92s 102ms/step - loss: 1.1617 - accuracy: 0.5737 - val_loss: 1.0731 - val_accuracy: 0.6158\n",
      "Epoch 39/40\n",
      "900/900 [==============================] - 90s 100ms/step - loss: 1.1525 - accuracy: 0.5797 - val_loss: 1.0663 - val_accuracy: 0.6205\n",
      "Epoch 40/40\n",
      "900/900 [==============================] - 93s 103ms/step - loss: 1.1587 - accuracy: 0.5778 - val_loss: 1.0646 - val_accuracy: 0.6219\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=40  # Ajusta el número de épocas según sea necesario\n",
    ")\n",
    "\n",
    "# Guardar el modelo\n",
    "model.save('Riley_8.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
